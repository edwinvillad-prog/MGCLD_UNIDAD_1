# app_bondad_ajuste.py
# Aplicaci√≥n profesional en Streamlit para pruebas de bondad de ajuste ‚Äî Posgrado

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chisquare, poisson, binom, hypergeom
from scipy.stats import kstest, norm, expon, lognorm
from scipy.stats import anderson

# =========================
# CONFIGURACI√ìN GENERAL
# =========================
st.set_page_config(
    page_title="Pruebas de Bondad de Ajuste ‚Äî Posgrado",
    page_icon="üìä",
    layout="wide"
)

st.title("üìä Pruebas de Bondad de Ajuste ‚Äî Posgrado")
st.markdown("### Maestr√≠a en Gesti√≥n de la Calidad de la Leche y sus Derivados")

# =========================
# SIDEBAR
# =========================
st.sidebar.header("Opciones de an√°lisis")
alpha = st.sidebar.selectbox("Nivel de significancia (Œ±)", [0.01, 0.05, 0.10], index=1)

# =========================
# SUBIDA DE ARCHIVO
# =========================
st.sidebar.markdown("### üìÇ Cargar archivo de datos")
uploaded_file = st.sidebar.file_uploader("Subir archivo CSV o Excel", type=["csv", "xlsx"])

data = None
if uploaded_file:
    if uploaded_file.name.endswith(".csv"):
        data = pd.read_csv(uploaded_file)
    else:
        data = pd.read_excel(uploaded_file)
    st.sidebar.success(f"Archivo cargado: {uploaded_file.name}")
else:
    st.sidebar.warning("‚ö†Ô∏è Por favor, cargue un archivo para habilitar los an√°lisis.")

# =========================
# PESTA√ëAS PRINCIPALES
# =========================
tabs = st.tabs([
    "üìò Teor√≠a",
    "üßÆ Chi-cuadrado (discretas)",
    "üìà Kolmogorov‚ÄìSmirnov (continuas)",
    "üìä Anderson‚ÄìDarling (continuas)"
])

# =========================
# TAB 1 ‚Äî TEOR√çA
# =========================
with tabs[0]:
    st.header("üìò Teor√≠a de Pruebas de Bondad de Ajuste")

    st.markdown("""
    ## üîπ 1. Chi-cuadrado de bondad de ajuste
    - **Tipo de prueba:** no param√©trica.  
    - **Condiciones:**  
      - Datos **discretos** (conteos o frecuencias).  
      - **Tama√±o muestral grande** (regla pr√°ctica: $n > 30$).  
      - **Frecuencia esperada m√≠nima:** cada clase debe tener $E_i \\geq 5$. Si no, agrupar categor√≠as.  
      - Se asume **independencia entre observaciones**.  
    - **Ventaja:** muy flexible, permite comparar con cualquier distribuci√≥n.  
    - **Limitaci√≥n:** depende del agrupamiento de clases, menos potente con muestras peque√±as.  
    - **Aplicaci√≥n en l√°cteos:** conteo de **unidades formadoras de colonias (UFC)** en placas, defectos en envases.  

    **F√≥rmula del estad√≠stico:**
    $$
    \\chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}, 
    $$

    ---

    ## üîπ 2. Kolmogorov‚ÄìSmirnov (KS)
    - **Tipo de prueba:** no param√©trica.  
    - **Condiciones:**  
      - Datos **continuos**.  
      - Adecuada para **muestras peque√±as o medianas** (p. ej. $n > 20$, aunque puede usarse con menos).  
      - Requiere que la **distribuci√≥n te√≥rica sea completamente especificada** (par√°metros conocidos). Si no, se usa la variante **Lilliefors**.  
      - Sensible a diferencias en la **parte central** de la distribuci√≥n, menos en las colas.  
    - **Ventaja:** no requiere agrupaci√≥n de datos.  
    - **Limitaci√≥n:** si los par√°metros se estiman de la muestra, el test pierde validez directa.  
    - **Aplicaci√≥n en l√°cteos:** verificar si la distribuci√≥n de la **grasa de la leche** sigue Normal, o si los niveles de prote√≠na siguen una distribuci√≥n esperada.  

    **F√≥rmula del estad√≠stico:**
    $$
    D = \\sup_x \\left| F_n(x) - F(x) \\right|,
    $$
    donde $F_n(x)$ es la funci√≥n de distribuci√≥n emp√≠rica y $F(x)$ la te√≥rica.  

    ---

    ## üîπ 3. Anderson‚ÄìDarling (AD)
    - **Tipo de prueba:** no param√©trica.  
    - **Condiciones:**  
      - Datos **continuos**.  
      - Recomendado para muestras **peque√±as y medianas** ($n \\geq 8$).  
      - Como en KS, los par√°metros de la distribuci√≥n te√≥rica deber√≠an estar especificados (aunque existen versiones ajustadas).  
      - Da mayor peso a las **colas** de la distribuci√≥n, lo que lo hace m√°s potente en detectar desviaciones extremas.  
    - **Ventaja:** m√°s sensible que KS cuando hay diferencias en colas.  
    - **Limitaci√≥n:** los valores cr√≠ticos dependen de la distribuci√≥n espec√≠fica (no universales como en KS).  
    - **Aplicaci√≥n en l√°cteos:** pruebas de **vida √∫til (tiempo hasta deterioro)** en productos fermentados (yogur, quesos frescos).  

    **F√≥rmula del estad√≠stico:**
    $$
    A^2 = -n - \\frac{1}{n} \\sum_{i=1}^n (2i-1) \\Big[ \\ln F(x_{(i)}) + \\ln(1-F(x_{(n+1-i)})) \\Big].
    $$

    ---

    ‚úÖ **Resumen comparativo:**  
    - **Chi¬≤** ‚Üí discreta, $n$ grande, requiere $E_i \\geq 5$.  
    - **KS** ‚Üí continua, par√°metros conocidos, sensible en el centro, v√°lido para muestras peque√±as.  
    - **AD** ‚Üí continua, m√°s potente que KS en colas, recomendado en vida √∫til y tiempos de falla.  
    """)
# =========================
# TAB 2 ‚Äî CHI-CUADRADO
# =========================
with tabs[1]:
    st.header("üßÆ Prueba Chi-cuadrado ‚Äî Ajuste de datos discretos")

    if data is not None:
        variable = st.selectbox("Seleccione variable discreta", data.columns, key="chi_var")
        dist = st.radio("Distribuci√≥n de referencia", ["Poisson", "Binomial", "Hipergeom√©trica"], key="chi_dist")

        valores = data[variable].value_counts().sort_index()
        n = valores.sum()

        if dist == "Poisson":
            lam = np.mean(data[variable])
            esperados = [poisson.pmf(k, lam) * n for k in range(len(valores))]
            dist_info = f"Poisson(Œª={lam:.2f})"

        elif dist == "Binomial":
            kmax = valores.index.max()
            p = np.mean(data[variable]) / kmax
            esperados = [binom.pmf(k, kmax, p) * n for k in range(len(valores))]
            dist_info = f"Binomial(n={kmax}, p={p:.2f})"

        else:  # Hipergeom√©trica (ejemplo simple con par√°metros de muestra)
            N = 100
            K = int(np.mean(data[variable]) * 2)
            n_s = int(np.median(data[variable]))
            esperados = [hypergeom.pmf(k, N, K, n_s) * n for k in range(len(valores))]
            dist_info = f"Hipergeom√©trica(N={N}, K={K}, n={n_s})"

        # Ajuste de normalizaci√≥n
        esperados = np.array(esperados) * (n / np.sum(esperados))

        chi2, pval = chisquare(valores, f_exp=esperados)

        st.write(f"**Distribuci√≥n de referencia:** {dist_info}")
        st.write(f"**Estad√≠stico Chi¬≤:** {chi2:.3f}")
        st.write(f"**p-valor:** {pval:.3f}")

        if pval < alpha:
             st.error(
                 f"‚ùå Se rechaza H‚ÇÄ al nivel Œ±={alpha}. "
                f"Esto indica que los datos observados **no se ajustan** a la distribuci√≥n {dist_info}, "
                " lo que sugiere posibles factores adicionales."
                )
        else:
                 st.success(
                 f"‚úÖ No se rechaza H‚ÇÄ al nivel Œ±={alpha}. "
                 f"Los datos pueden considerarse consistentes con la distribuci√≥n {dist_info}, "
                 "por lo que el modelo es adecuado en este contexto."
                )

        fig, ax = plt.subplots(figsize=(6,4), dpi=120)
        ax.bar(valores.index, valores, alpha=0.6, label="Observados")
        ax.plot(valores.index, esperados, "ro--", label="Esperados")
        ax.set_title("Chi¬≤: Observados vs Esperados")
        ax.set_xlabel("Clases")
        ax.set_ylabel("Frecuencia")
        ax.legend()
        st.pyplot(fig)

# =========================
# TAB 3 ‚Äî KS
# =========================
with tabs[2]:
    st.header("üìà Prueba Kolmogorov‚ÄìSmirnov ‚Äî Ajuste de datos continuos")

    if data is not None:
        variable = st.selectbox("Seleccione variable continua (KS)", data.columns, key="ks_var")
        dist = st.radio("Distribuci√≥n de referencia", ["Normal", "Exponencial", "Lognormal"], key="ks_dist")

        x = data[variable].dropna().values

        if dist == "Normal":
            mu, sigma = np.mean(x), np.std(x, ddof=1)
            D, pval = kstest(x, "norm", args=(mu, sigma))
            dist_info = f"Normal(Œº={mu:.2f}, œÉ={sigma:.2f})"
            cdf_theoretical = norm.cdf(np.sort(x), mu, sigma)

        elif dist == "Exponencial":
            lam = 1 / np.mean(x)
            D, pval = kstest(x, "expon", args=(0, 1/lam))
            dist_info = f"Exponencial(Œª={lam:.2f})"
            cdf_theoretical = expon.cdf(np.sort(x), scale=1/lam)

        else:  # Lognormal
            shape = np.std(np.log(x), ddof=1)
            scale = np.exp(np.mean(np.log(x)))
            D, pval = kstest(x, "lognorm", args=(shape, 0, scale))
            dist_info = f"Lognormal(shape={shape:.2f}, scale={scale:.2f})"
            cdf_theoretical = lognorm.cdf(np.sort(x), shape, 0, scale)

        st.write(f"**Distribuci√≥n de referencia:** {dist_info}")
        st.write(f"**Estad√≠stico D:** {D:.3f}")
        st.write(f"**p-valor:** {pval:.3f}")

        

        if pval < alpha:
             st.error(f"‚ùå Se rechaza H‚ÇÄ al nivel Œ±={alpha}. "
            f"Esto significa que los datos **no siguen** la distribuci√≥n {dist_info} " 
            "por lo que el modelo no es adecuado en este contexto.")
        else:
                st.success(
            f"‚úÖ No se rechaza H‚ÇÄ al nivel Œ±={alpha}. "
            f"Los datos se ajustan con la distribuci√≥n {dist_info} "
        "por lo que el modelo es adecuado en este contexto."
        )

        fig, ax = plt.subplots(figsize=(6,4), dpi=120)
        ecdf = np.arange(1, len(x)+1) / len(x)
        ax.step(np.sort(x), ecdf, where="post", label="CDF emp√≠rica")
        ax.plot(np.sort(x), cdf_theoretical, "r--", label=f"CDF {dist}")
        ax.set_title("Prueba KS")
        ax.set_xlabel("x")
        ax.set_ylabel("Probabilidad acumulada")
        ax.legend()
        st.pyplot(fig)

# =========================
# TAB 4 ‚Äî AD
# =========================
with tabs[3]:
    st.header("üìä Prueba Anderson‚ÄìDarling ‚Äî Ajuste de datos continuos")

    if data is not None:
        variable = st.selectbox("Seleccione variable continua (AD)", data.columns, key="ad_var")
        dist = st.radio("Distribuci√≥n de referencia", ["Normal", "Exponencial", "Lognormal"], key="ad_dist")

        x = data[variable].dropna().values
        resultado = anderson(x, dist="norm" if dist=="Normal" else "expon")

        # Elegir valor cr√≠tico correspondiente a Œ±
        niveles = resultado.significance_level / 100
        idx = (np.abs(niveles - alpha)).argmin()
        critico = resultado.critical_values[idx]

        st.write(f"**Distribuci√≥n de referencia:** {dist}")

        st.write(f"**Estad√≠stico A¬≤:** {resultado.statistic:.3f}")

        # Buscar el valor cr√≠tico m√°s cercano al Œ± elegido
        niveles = resultado.significance_level / 100
        idx = (np.abs(niveles - alpha)).argmin()
        valor_critico = resultado.critical_values[idx]

        st.write(f"**Valor cr√≠tico al {alpha*100:.0f}%:** {valor_critico:.3f}")

        if resultado.statistic > valor_critico:
            st.error(
        f"‚ùå Se rechaza H‚ÇÄ al nivel Œ±={alpha}. "
        f"Los datos muestran desviaciones significativas respecto a la distribuci√≥n {dist}, "
        "especialmente en las colas."
          )
        else:
             st.success(
        f"‚úÖ No se rechaza H‚ÇÄ al nivel Œ±={alpha}. "
        f"Los datos no presentan diferencias significativas respecto a la distribuci√≥n {dist}, "
        "lo que valida el modelo asumido."
         )

        # Gr√°fico
        fig, ax = plt.subplots(figsize=(6,4), dpi=120)
        ax.hist(x, bins=15, density=True, alpha=0.6, edgecolor="black", label="Datos")
        xx = np.linspace(min(x), max(x), 200)
        if dist == "Normal":
            ax.plot(xx, norm.pdf(xx, np.mean(x), np.std(x, ddof=1)), "r--", lw=2, label="Normal")
        elif dist == "Exponencial":
            lam = 1 / np.mean(x)
            ax.plot(xx, expon.pdf(xx, scale=1/lam), "r--", lw=2, label="Exponencial")
        else:
            shape = np.std(np.log(x), ddof=1)
            scale = np.exp(np.mean(np.log(x)))
            ax.plot(xx, lognorm.pdf(xx, shape, 0, scale), "r--", lw=2, label="Lognormal")
        ax.set_title("Prueba Anderson‚ÄìDarling")
        ax.set_xlabel("x")
        ax.set_ylabel("Densidad")
        ax.legend()
        st.pyplot(fig)
